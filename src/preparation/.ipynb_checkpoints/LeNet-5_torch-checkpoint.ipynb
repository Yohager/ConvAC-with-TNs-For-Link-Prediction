{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MINST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MINST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MINST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MINST_data/t10k-labels-idx1-ubyte.gz\n",
      "Image shape: (28, 28, 1)\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "minst_data = input_data.read_data_sets('MINST_data/',reshape=False)\n",
    "X_train, y_train = minst_data.train.images, minst_data.train.labels\n",
    "X_validation, y_validation = minst_data.validation.images, minst_data.validation.labels\n",
    "X_test, y_test = minst_data.test.images, minst_data.test.labels\n",
    "\n",
    "assert (len(X_train) == len(y_train))\n",
    "assert (len(X_validation) == len(y_validation))\n",
    "assert (len(X_test) == len(y_test))\n",
    "\n",
    "print(\"Image shape: {}\".format(X_train[0].shape))\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Image Shape: (32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.pad(X_train,((0,0),(2,2),(2,2),(0,0)),'constant')\n",
    "X_validation = np.pad(X_validation,((0,0),(2,2),(2,2),(0,0)),'constant')\n",
    "X_test = np.pad(X_test,((0,0),(2,2),(2,2),(0,0)),'constant')\n",
    "\n",
    "print(\"Updated Image Shape: {}\".format(X_train[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2228dc72fd0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "#查看图片\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = random.randint(0,len(X_train))\n",
    "image = X_train[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image,cmap='gray')\n",
    "plt.show()\n",
    "print(y_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#打乱数据\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "'''\n",
    "input: 32*32*c \n",
    "结构：\n",
    "layer1:卷积, output_size: 28*28*6\n",
    "activation: relu\n",
    "pooling: output_size: 14*14*6\n",
    "layer2:卷积, output_size: 10*10*6\n",
    "activation: relu\n",
    "pooling: output_size: 5*5*16\n",
    "flatten\n",
    "layer3:Full Connected 120 outputs\n",
    "activation: relu\n",
    "layer4:Full Connected 84 outputs\n",
    "activation: relu\n",
    "layer5:Full Connected 10 outputs\n",
    "'''\n",
    "\n",
    "from  tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x):\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    conv1_w = tf.Variable(tf.truncated_normal(shape=(5,5,1,6),mean=mu,stddev=sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1   = tf.nn.conv2d(x,conv1_w,strides=[1,1,1,1],padding='VALID') + conv1_b\n",
    "    \n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    \n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "    \n",
    "    conv2_w = tf.Variable(tf.truncated_normal(shape=(5,5,6,16),mean=mu,stddev=sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2   = tf.nn.conv2d(conv1,conv2_w,strides=[1,1,1,1],padding='VALID') + conv2_b\n",
    "    \n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    \n",
    "    fc0 = flatten(conv2)\n",
    "    \n",
    "    fc1_w = tf.Variable(tf.truncated_normal(shape=(400,120),mean=mu,stddev=sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1   = tf.matmul(fc0,fc1_w) + fc1_b\n",
    "    \n",
    "    fc1   = tf.nn.relu(fc1)\n",
    "\n",
    "    fc2_w = tf.Variable(tf.truncated_normal(shape=(120,84),mean=mu,stddev=sigma))\n",
    "    fc2_b = tf.Variable(tf.zeros(84))\n",
    "    fc2   = tf.matmul(fc1,fc2_w) + fc2_b\n",
    "    \n",
    "    fc2   = tf.nn.relu(fc2)\n",
    "\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 10), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(10))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,(None,32,32,1))\n",
    "y = tf.placeholder(tf.int32,(None))\n",
    "one_hot_y = tf.one_hot(y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-12-ae28af164121>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training pipline\n",
    "rate = 0.001\n",
    "logits = LeNet(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y,logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.970\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.982\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.983\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Accuracy = 0.984\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Accuracy = 0.987\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation Accuracy = 0.988\n",
      "\n",
      "EPOCH 7 ...\n",
      "Validation Accuracy = 0.988\n",
      "\n",
      "EPOCH 8 ...\n",
      "Validation Accuracy = 0.985\n",
      "\n",
      "EPOCH 9 ...\n",
      "Validation Accuracy = 0.989\n",
      "\n",
      "EPOCH 10 ...\n",
      "Validation Accuracy = 0.988\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_validation, y_validation)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, './lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from .\\lenet\n",
      "Test Accuracy = 0.988\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    \n",
    "    test_accuracy = evaluate(X_test,y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
